{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core, cv2, keras, io, json, ntpath, os, requests, urllib\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core import Datastore, Experiment, ScriptRunConfig, Workspace\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE, RunConfiguration\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = os.path.join(Path(os.getcwd()).parent, 'data')\n",
    "x_train_dir = os.path.join(data_folder_path, 'raw')\n",
    "y_train_dir = os.path.join(data_folder_path, 'processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Masks from Labelbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_file_path = os.path.join(data_folder_path, 'export-2019-10-14T06_16_18.335Z.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(export_file_path, 'r') as export_file:\n",
    "    data = json.load(export_file)\n",
    "    for row in data:\n",
    "        for object in row['Label']['objects']:\n",
    "            response = requests.get(object['instanceURI'])\n",
    "            image = Image.open(io.BytesIO(response.content))\n",
    "            image = image.convert(\"RGB\")\n",
    "            image.save(os.path.join(y_train_dir, '{0}_{1}.JPG'.format(row['External ID'].split('.')[0], object['value'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "# helper function for data visualization    \n",
    "def denormalize(x):\n",
    "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
    "    x_max = np.percentile(x, 98)\n",
    "    x_min = np.percentile(x, 2)    \n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    x = x.clip(0, 1)\n",
    "    return x\n",
    "    \n",
    "\n",
    "# classes for data loading and preprocessing\n",
    "class Dataset:\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['other_grass', 'para_grass', 'tree']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # add background if mask is not binary\n",
    "        if mask.shape[-1] != 1:\n",
    "            background = 1 - mask.sum(axis=-1, keepdims=True)\n",
    "            mask = np.concatenate((mask, background), axis=-1)\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    \n",
    "class Dataloder(keras.utils.Sequence):\n",
    "    \"\"\"Load data from dataset and form batches\n",
    "    \n",
    "    Args:\n",
    "        dataset: instance of Dataset class for image loading and preprocessing.\n",
    "        batch_size: Integet number of images in batch.\n",
    "        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(dataset))\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "        \n",
    "        # transpose list of lists\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.indexes) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at data we have\n",
    "dataset = Dataset(x_train_dir, y_train_dir, classes=['other_grass', 'para_grass', 'tree'])\n",
    "\n",
    "image, mask = dataset[5] # get some sample\n",
    "visualize(\n",
    "    image=image, \n",
    "    cars_mask=mask[..., 0].squeeze(),\n",
    "    sky_mask=mask[..., 1].squeeze(),\n",
    "    background_mask=mask[..., 2].squeeze(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print('Name: {0}'.format(ws.name), 'Resource Group: {0}'.format(ws.resource_group), 'Location: {0}'.format(ws.location), 'Subscription Id: {0}'.format(ws.subscription_id), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_name = 'CPU'\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    \n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('Found compute target: ' + compute_name)\n",
    "else:\n",
    "    provisioning_configuration = AmlCompute.provisioning_configuration(vm_size = 'STANDARD_D2_V2',\n",
    "                                                                min_nodes = 1,\n",
    "                                                                max_nodes = 2)\n",
    "\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_configuration)\n",
    "    \n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_file_store = ws.get_default_datastore() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = Path(os.getcwd()).parent\n",
    "print(parent_folder)\n",
    "\n",
    "data_path = os.path.join(parent_folder, 'data')\n",
    "print(data_path)\n",
    "raw_data_path = os.path.join(data_path, 'raw')\n",
    "print(raw_data_path)\n",
    "src_path = os.path.join(parent_folder, 'src')\n",
    "print(src_path)\n",
    "tools_path = os.path.join(parent_folder, 'tools')\n",
    "print(tools_path)\n",
    "\n",
    "source_directory = os.path.join(src_path, 'FishOrNoFish')\n",
    "print(source_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tools_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(tools_path):\n",
    "    for file in files:\n",
    "        tools_files.append(os.path.join(root, file))\n",
    "        \n",
    "default_file_store.upload_files(files=tools_files,\n",
    "                                target_path='tools/ffmpeg-4.1.3-win64-static',\n",
    "                                overwrite=False,\n",
    "                                show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(raw_data_path):\n",
    "    for file in files:\n",
    "        if '.MP4' in file and '_VIDEO' not in file and 'Frames' not in root:\n",
    "            file_path = os.path.join(root, file)\n",
    "            video_name = ntpath.basename(file_path)\n",
    "            target_path = os.sep.join(file_path.split(os.sep)[3:-1])\n",
    "            file_or_dirs = file_service_list_directories_and_files(account_name, storage_key, share_name, target_path)\n",
    "            if video_name not in file_or_dirs:\n",
    "                print('Uploading {0}'.format(file_path))\n",
    "                default_file_store.upload_files([file_path], target_path=target_path, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_on_datastore = os.sep.join(raw_data_path.split(os.sep)[3:]).replace('\\\\', '/')\n",
    "\n",
    "raw_data_reference = DataReference(datastore=default_file_store,\n",
    "                                   data_reference_name='raw_data',\n",
    "                                   path_on_datastore=path_on_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_on_datastore = os.sep.join(tools_path.split(os.sep)[3:]).replace('\\\\', '/')\n",
    "\n",
    "tools_reference = DataReference(datastore=default_file_store,\n",
    "                                   data_reference_name='tools',\n",
    "                                   path_on_datastore=path_on_datastore)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "frames_path = os.path.join(target_path, 'Frames')\n",
    "\n",
    "frames_path = frames_path.replace('\\\\', '/')\n",
    "\n",
    "frames = DataReference(datastore=default_file_store,\n",
    "                       data_reference_name='frames_data',\n",
    "                       path_on_datastore=frames_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_dependencies = CondaDependencies()\n",
    "conda_dependencies.add_conda_package('opencv')\n",
    "\n",
    "run_configuration = RunConfiguration()\n",
    "run_configuration.environment.docker.enabled = True\n",
    "run_configuration.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_configuration.environment.python.user_managed_dependencies = False\n",
    "run_configuration.environment.python.conda_dependencies = conda_dependencies\n",
    "run_configuration.target = compute_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_video_step = PythonScriptStep(name='extract_video',\n",
    "                                      source_directory=source_directory,\n",
    "                                      script_name='extract_video.py',\n",
    "                                      arguments=['--raw_data_path', raw_data_reference, '--tools_path', tools_reference],\n",
    "                                      inputs=[raw_data_reference, tools_reference],\n",
    "                                      runconfig=run_configuration,\n",
    "                                      allow_reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[extract_video_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(ws, 'extract_video').submit(pipeline)\n",
    "pipeline_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames_step = PythonScriptStep(name='extract_frames',\n",
    "                                       source_directory=source_directory,\n",
    "                                       script_name='extract_frames.py',\n",
    "                                       arguments=['--raw_data', videos, '--raw_frames', frames, '--x', 2],\n",
    "                                       inputs=[videos, frames],\n",
    "                                       #outputs=[frames],\n",
    "                                       runconfig=run_configuration,\n",
    "                                       allow_reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[extract_frames_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(ws, 'extract_frames').submit(pipeline)\n",
    "pipeline_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = pipeline_run.get_children()\n",
    "for child in children:\n",
    "    status = child.get_status()\n",
    "    print('Id:', child.id, 'Script:', child.name, 'Status:', status)\n",
    "    RunDetails(child).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "\n",
    "top = 'C:\\\\Source\\\\FishyBusiness\\\\data\\\\raw\\\\Channels 2017\\\\Mudginberri 2017\\\\Transect 1\\\\Location 1'\n",
    "frames = 'C:\\\\Source\\\\FishyBusiness\\\\data\\\\raw\\\\Channels 2017\\\\Mudginberri 2017\\\\Transect 1\\\\Location 1\\\\Frames'\n",
    "x = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(top):\n",
    "    for video in files:\n",
    "        if '.MP4' in video:\n",
    "            print(video)\n",
    "            video_path = os.path.join(top, video)\n",
    "            video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "            frame_rate = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "            \n",
    "            frame_position = 0\n",
    "            \n",
    "            result, frame = video_capture.read()\n",
    "            \n",
    "            while result:\n",
    "                try:\n",
    "                    frame_path = os.path.join(frames, video + '_frame_%d.jpg' % frame_position)\n",
    "                    \n",
    "                    print('Writing frame %s' % (frame_path))\n",
    "                    cv2.imwrite(frame_path, frame)\n",
    "                    \n",
    "                    frame_position = frame_position + (int(frame_rate) * x)\n",
    "                    print(frame_position)\n",
    "                    \n",
    "                    video_capture.set(cv2.CAP_PROP_POS_FRAMES, frame_position)\n",
    "                    result, frame = video_capture.read()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            print('Exporting finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(top):\n",
    "    for file in files:\n",
    "        if '.MP4' in file and '_VIDEO' not in file and 'Frames' not in root:\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "\n",
    "for file_path in file_paths:\n",
    "    video_name = ntpath.basename(file_path)\n",
    "    print(video_name)\n",
    "    file_parts = video_name.split('.')\n",
    "    print(file_parts)\n",
    "    path_parts = file_path.split(os.sep)[:-1]\n",
    "    print(path_parts)\n",
    "    path_parts.append('{0}_VIDEO.{1}'.format(file_parts[0], file_parts[1]))\n",
    "    out_path = os.sep.join(path_parts)\n",
    "    print(out_path)\n",
    "    ffmpeg_exe_path = os.path.join('..', 'src', 'FishOrNoFish', 'ffmpeg-4.1.3-win64-static', 'bin', 'ffmpeg.exe')\n",
    "    print(ffmpeg_exe_path)\n",
    "    ffmpeg_command = '{0} -loglevel \"verbose\" -i \"{1}\" -c copy -an \"{2}\"'.format(ffmpeg_exe_path, file_path, out_path)\n",
    "    print(ffmpeg_command)\n",
    "    os.system(ffmpeg_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
